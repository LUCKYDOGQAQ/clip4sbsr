setting:
  seed: 42
  wandb:
    project: CLIP4SBSR_v2
    name: 13_lora_softmax_v2
dataset:
  train_sketch_datadir: './data/SHREC13_ZS2/13_sketch_train_picture'
  train_view_datadir: './data/SHREC13_ZS2/13_view_render_train_img'
  test_sketch_datadir: './data/SHREC13_ZS2/13_sketch_test_picture'
  test_view_datadir: './data/SHREC13_ZS2/13_view_render_test_img'
  num_workers: 6
  batch_size: 32
model:
  backbone: "./hf_model/models--openai--clip-vit-base-patch32"
  save_path: './ckpt/lora_epx/13_lora_softmax'
  lr_model: 1.0e-4
  loss_type: "softmax"
  classifier:
    alph: 12
    feat_dim: 512
    num_classes: 133
  lora:
    use_lora: True
    lora_rank: 32
  prompt:
    use_prompt: False
    shared_prompt: False
    num_prompts: 3
    prompt_dim: 768 # dim_embedding
    lr_prompt: 1.0e-3

trainer:
  max_epochs: 30
  gpu: '0,1,2,3'
  count: 0

